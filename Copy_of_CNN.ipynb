{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGBwYIiag_Tt",
        "outputId": "7ad4ab4a-a477-4b0c-d014-8da361c2af4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting np_utils\n",
            "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from np_utils) (1.23.5)\n",
            "Building wheels for collected packages: np_utils\n",
            "  Building wheel for np_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np_utils: filename=np_utils-0.6.0-py3-none-any.whl size=56439 sha256=8e5f1dbfb9b77301504ad4e77590dea420021339e3ad64faeb443b5e2689b19c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/c7/50/2307607f44366dd021209f660045f8d51cb976514d30be7cc7\n",
            "Successfully built np_utils\n",
            "Installing collected packages: np_utils\n",
            "Successfully installed np_utils-0.6.0\n",
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install np_utils\n",
        "!pip install tensorflow-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO7H5G7zoy5P"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnANBvkUJk_3",
        "outputId": "23943fb1-c19d-418d-d0e2-03ead5b92a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 7887937171175726508\n",
            "xla_global_id: -1\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmBBvy3IRnNA"
      },
      "outputs": [],
      "source": [
        "# #importing libraries\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# # #tf.enable_eager_execution()\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import Adam\n",
        "import np_utils\n",
        "from keras.models import Sequential\n",
        "# # from keras.preprocessing.image import ImageDataGenerator\n",
        "# # # from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "# # # import matplotlib.image as mpimg\n",
        "# # # import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "from random import sample\n",
        "from numpy import *\n",
        "# # from PIL import Image\n",
        "\n",
        "# # #import theano"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6rqJWwkXLci"
      },
      "outputs": [],
      "source": [
        "#Prepare Dataset for Training\n",
        "path_train = \"/content/drive/MyDrive/IP/image_train_test/train\"\n",
        "CATEGORIES = [\"agreeableness\",\"conscientiousness\",\"extraversion\",\"neuroticism\",\"openness\"]\n",
        "#print(img_array.shape)\n",
        "#new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "IMG_SIZE =200\n",
        "training = []\n",
        "\n",
        "#folder_path = \"path/to/folder\"\n",
        "#num_files = 5  # specify the number of files to select\n",
        "\n",
        "# all_files = os.listdir(folder_path)\n",
        "# random_files = random.sample(all_files, num_files)\n",
        "\n",
        "# print(\"Randomly selected files:\")\n",
        "# for file in random_files:\n",
        "#     print(os.path.join(folder_path, file))\n",
        "\n",
        "\n",
        "def createTrainingData():\n",
        "  for category in CATEGORIES:\n",
        "    path = os.path.join(path_train, category)\n",
        "    class_num = CATEGORIES.index(category)\n",
        "    num_files = 200  # specify the number of files to select\n",
        "    all_files = os.listdir(path)\n",
        "    random_files = sample(all_files, num_files)\n",
        "    j=1\n",
        "    for img in random_files:\n",
        "      if j<=200:\n",
        "        img_array = cv2.imread(os.path.join(path,img))\n",
        "        new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "        training.append([new_array, class_num])\n",
        "        j+=1\n",
        "        print(j)\n",
        "\n",
        "createTrainingData()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbFzpgxobE0c"
      },
      "outputs": [],
      "source": [
        "#Prepare Dataset for Testing\n",
        "path_test = \"/content/drive/MyDrive/IP/image_train_test/test\"\n",
        "#CATEGORIES = [\"agreeableness\",\"conscientiousness\",\"extraversion\",\"neuroticism\",\"openness\"]\n",
        "#print(img_array.shape)\n",
        "#new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "#IMG_SIZE =200\n",
        "testing = []\n",
        "def createTestingData():\n",
        "  for category in CATEGORIES:\n",
        "    path = os.path.join(path_test, category)\n",
        "    class_num = CATEGORIES.index(category)\n",
        "    num_files = 60  # specify the number of files to select\n",
        "    all_files = os.listdir(path)\n",
        "    random_files = sample(all_files, num_files)\n",
        "    j=1\n",
        "    for img in random_files:   #os.listdir(path)\n",
        "      if j<=60:\n",
        "        img_array = cv2.imread(os.path.join(path,img))\n",
        "        new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "        testing.append([new_array, class_num])\n",
        "        j+=1\n",
        "        print(j)\n",
        "createTestingData()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-lkkUIk_Qu4",
        "outputId": "d5983cb6-a174-484c-cd74-f4326c22a673"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "len(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DIzHPVC_Vsh",
        "outputId": "7549298c-aa08-4b8d-d3df-34aa48836a82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BAFxxnMZI6f"
      },
      "outputs": [],
      "source": [
        "#Shuffle the Training Dataset\n",
        "random.shuffle(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r_tAvuybkHy"
      },
      "outputs": [],
      "source": [
        "#Shuffle the Testing Dataset\n",
        "random.shuffle(testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8RsdRnuX1Kg"
      },
      "outputs": [],
      "source": [
        "#Assigning Labels and Features for training dataset\n",
        "X_train =[]\n",
        "y_train =[]\n",
        "for features, label in training:\n",
        "  X_train.append(features)\n",
        "  y_train.append(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1UG7r4wtZlA"
      },
      "outputs": [],
      "source": [
        "X_train = np.array(X_train).reshape(-1, IMG_SIZE, IMG_SIZE, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FdxbVVDb9dP"
      },
      "outputs": [],
      "source": [
        "#Assigning Labels and Features for Testing dataset\n",
        "X_test =[]\n",
        "y_test =[]\n",
        "for features, label in testing:\n",
        "  X_test.append(features)\n",
        "  y_test.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BGNS97etfhz"
      },
      "outputs": [],
      "source": [
        "X_test = np.array(X_test).reshape(-1, IMG_SIZE, IMG_SIZE, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7iRGaexX4pE"
      },
      "outputs": [],
      "source": [
        "#Normalising X and converting labels to categorical data for Training Dataset\n",
        "X_train = X_train.astype('float32')\n",
        "X_train /= 255\n",
        "\n",
        "#X_train = (X_train / 255.0).astype('float16')\n",
        "\n",
        "\n",
        "# # create a data generator for image normalization\n",
        "# datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# # fit the data generator on the training data\n",
        "# datagen.fit(X_train)\n",
        "\n",
        "# # use the data generator to normalize the training data in batches\n",
        "# X_train = datagen.flow(X_train, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHBH_UJRtmQo",
        "outputId": "b7385ad9-e902-452e-e87e-b070e582452a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 1.]\n",
            "(1000, 5)\n"
          ]
        }
      ],
      "source": [
        "# from keras.utils import np_utils\n",
        "Y_train = to_categorical(y_train, 5)\n",
        "print(Y_train[100])\n",
        "print(shape(Y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sppde9C_cfAE"
      },
      "outputs": [],
      "source": [
        "#Normalising X and converting labels to categorical data for Testing Dataset\n",
        "X_test = X_test.astype('float32')\n",
        "X_test /= 255\n",
        "\n",
        "#X_test = (X_test / 255.0).astype('float16')\n",
        "\n",
        "# # create a data generator for image normalization\n",
        "# datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# # fit the data generator on the training data\n",
        "# datagen.fit(X_test)\n",
        "\n",
        "# # use the data generator to normalize the training data in batches\n",
        "# X_test = datagen.flow(X_test, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STHR9pu5umef",
        "outputId": "ea2f4a34-f019-445d-dcb4-d9baec2703e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 1.]\n",
            "(300, 5)\n"
          ]
        }
      ],
      "source": [
        "# from keras.utils import np_utils\n",
        "Y_test = to_categorical(y_test, 5)\n",
        "print(Y_test[33])\n",
        "print(shape(Y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zob56kGHD2cv",
        "outputId": "758fc838-9004-438c-b786-5516b93db1ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 1.]\n"
          ]
        }
      ],
      "source": [
        "print(Y_test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLgEdUAYX7HO"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "nb_classes = 5\n",
        "nb_epochs = 10\n",
        "img_rows, img_columns = 200, 200\n",
        "img_channel = 3\n",
        "nb_filters = 32\n",
        "nb_pool = 2\n",
        "nb_conv = 3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMjwgiWNZfBJ"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
        "                           input_shape=(200, 200, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation=tf.nn.relu,kernel_regularizer=regularizers.l2(0.01)),\n",
        "    tf.keras.layers.Dense(5,  activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuVTMZqRaw-Y"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, Y_train, batch_size = batch_size, epochs = nb_epochs, verbose = 1, validation_data = (X_test, Y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vww9Oja1yC8m",
        "outputId": "26fa5f97-f78c-4338-a30b-27b267f906d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 198, 198, 32)      896       \n",
            "                                                                 \n",
            " batch_normalization_10 (Ba  (None, 198, 198, 32)      128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPoolin  (None, 99, 99, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 97, 97, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_11 (Ba  (None, 97, 97, 64)        256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPoolin  (None, 48, 48, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 46, 46, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_12 (Ba  (None, 46, 46, 128)       512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPoolin  (None, 23, 23, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 67712)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               17334528  \n",
            "                                                                 \n",
            " batch_normalization_13 (Ba  (None, 256)               1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_14 (Ba  (None, 128)               512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17463749 (66.62 MB)\n",
            "Trainable params: 17462533 (66.61 MB)\n",
            "Non-trainable params: 1216 (4.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# from tensorflow.keras.applications import ResNet50\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "\n",
        "# base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\n",
        "\n",
        "# model = Sequential([\n",
        "#     base_model,\n",
        "#     GlobalAveragePooling2D(),\n",
        "#     Dense(128, activation='relu'),\n",
        "#     Dense(5, activation='softmax')\n",
        "# ])\n",
        "\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
        "\n",
        "# # Model architecture\n",
        "# model = Sequential([\n",
        "#     # Convolutional layers\n",
        "#     Conv2D(32, (3, 3), activation='relu', input_shape=(200, 200, 3)),\n",
        "#     BatchNormalization(),\n",
        "#     MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "#     Conv2D(64, (3, 3), activation='relu'),\n",
        "#     BatchNormalization(),\n",
        "#     MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "#     Conv2D(128, (3, 3), activation='relu'),\n",
        "#     BatchNormalization(),\n",
        "#     MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "#     # Flatten layer\n",
        "#     Flatten(),\n",
        "\n",
        "#     # Dense (fully connected) layers\n",
        "#     Dense(256, activation='relu'),\n",
        "#     BatchNormalization(),\n",
        "#     Dropout(0.5),\n",
        "\n",
        "#     Dense(128, activation='relu'),\n",
        "#     BatchNormalization(),\n",
        "#     Dropout(0.5),\n",
        "\n",
        "#     # Output layer\n",
        "#     Dense(5, activation='softmax')\n",
        "# ])\n",
        "\n",
        "# # Model compilation\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Model architecture\n",
        "model = Sequential([\n",
        "    # Convolutional layers\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(200, 200, 3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Flatten layer\n",
        "    Flatten(),\n",
        "\n",
        "    # Dense (fully connected) layers with dropout and L2 regularization\n",
        "    Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Output layer\n",
        "    Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "# Model compilation\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_FXfIP3yVA7",
        "outputId": "157981e6-f502-4dda-b045-07f4bff7471f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 147s 4s/step - loss: 11.8154 - accuracy: 0.2350 - val_loss: 12.4742 - val_accuracy: 0.1867\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 137s 4s/step - loss: 11.2905 - accuracy: 0.3570 - val_loss: 10.9376 - val_accuracy: 0.2000\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 136s 4s/step - loss: 8.8555 - accuracy: 0.4640 - val_loss: 8.5260 - val_accuracy: 0.2000\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 137s 4s/step - loss: 7.5150 - accuracy: 0.5080 - val_loss: 8.3739 - val_accuracy: 0.2000\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 146s 5s/step - loss: 6.9684 - accuracy: 0.5030 - val_loss: 7.1632 - val_accuracy: 0.2000\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 137s 4s/step - loss: 6.0180 - accuracy: 0.5920 - val_loss: 6.6281 - val_accuracy: 0.2000\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 147s 5s/step - loss: 5.5026 - accuracy: 0.6550 - val_loss: 6.5915 - val_accuracy: 0.2167\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 147s 5s/step - loss: 5.8710 - accuracy: 0.6350 - val_loss: 6.6347 - val_accuracy: 0.1700\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 146s 5s/step - loss: 5.8328 - accuracy: 0.7010 - val_loss: 6.6315 - val_accuracy: 0.2000\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 138s 4s/step - loss: 6.1130 - accuracy: 0.6990 - val_loss: 7.1888 - val_accuracy: 0.1833\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 136s 4s/step - loss: 5.9674 - accuracy: 0.7220 - val_loss: 6.9638 - val_accuracy: 0.2200\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 148s 5s/step - loss: 5.6322 - accuracy: 0.7550 - val_loss: 6.7450 - val_accuracy: 0.2133\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 149s 5s/step - loss: 5.5062 - accuracy: 0.7660 - val_loss: 6.7375 - val_accuracy: 0.2000\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 146s 5s/step - loss: 5.2895 - accuracy: 0.7880 - val_loss: 6.5396 - val_accuracy: 0.2067\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 149s 5s/step - loss: 4.8072 - accuracy: 0.8100 - val_loss: 6.1271 - val_accuracy: 0.2333\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 153s 5s/step - loss: 4.5381 - accuracy: 0.8050 - val_loss: 6.0980 - val_accuracy: 0.2067\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 147s 5s/step - loss: 5.0099 - accuracy: 0.7870 - val_loss: 6.8520 - val_accuracy: 0.2033\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 147s 5s/step - loss: 5.1921 - accuracy: 0.8100 - val_loss: 6.6114 - val_accuracy: 0.1933\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 147s 5s/step - loss: 4.6344 - accuracy: 0.8410 - val_loss: 6.0082 - val_accuracy: 0.2000\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 147s 5s/step - loss: 5.3592 - accuracy: 0.7410 - val_loss: 7.5676 - val_accuracy: 0.1967\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fec1e627bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "model.fit(X_train, Y_train,\n",
        "          epochs=20,\n",
        "          batch_size=32,\n",
        "          validation_data=((X_test, Y_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us7YXd7Ed36Z",
        "outputId": "11647817-8483-495c-e7a3-a5df47007233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy:  [1.2789645195007324, 0.8769999742507935]\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose = 0 )\n",
        "print(\"Test accuracy: \", score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdzxEfJ-ratr"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL_9zzCEucbK",
        "outputId": "88ae3d6e-2499-46dd-b3a4-73ed5c633b55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 1.] [0. 0. 1. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "print(predictions[1],Y_test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zMI36kvrbg7",
        "outputId": "580ae17b-6b01-44e8-c960-15161e5adb8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.2641620037303491\n",
            "Recall: 0.20666666666666667\n",
            "Accuracy: 0.20666666666666667\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(X_test)\n",
        "predictions = to_categorical(predictions.argmax(axis=1), num_classes=5)\n",
        "precision = precision_score(Y_test, predictions,average='weighted')\n",
        "recall = recall_score(Y_test, predictions,average='weighted')\n",
        "accuracy = accuracy_score(Y_test, predictions)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjajrAYrsIPU",
        "outputId": "17aecdcc-4afd-4c51-af59-7656e808f4ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Y_test shape: (1000, 5)\n",
            "Predictions shape: (1000, 5)\n"
          ]
        }
      ],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7MH_WJD4XyY"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# with open(\"/content/drive/MyDrive/IP/models/image_model.pkl\", \"wb\") as f:\n",
        "#     pickle.dump(model, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJbel6ix8w-T",
        "outputId": "e81bab7b-7d96-479b-94ba-2b980dff73fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 200, 200, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 100, 100, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 100, 100, 32)      9248      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 50, 50, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 50, 50, 32)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 80000)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               10240128  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,250,917\n",
            "Trainable params: 10,250,917\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld0uPeWZqLNq"
      },
      "outputs": [],
      "source": [
        "model.save(\"/content/drive/MyDrive/IP/models/image_model.h5\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}